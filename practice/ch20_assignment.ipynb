{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c24a0e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/JAE111/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.snowball import EnglishStemmer \n",
    "import matplotlib.pylab as plt\n",
    "from dmba import printTermDocumentMatrix, classificationSummary, liftChart\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c7775",
   "metadata": {},
   "source": [
    "# 20.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd4444",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc2c43",
   "metadata": {},
   "source": [
    "1. !\n",
    "2. <br /> \n",
    "3. <font size=\"3> \n",
    "4. &qout; \n",
    "5. </font>\n",
    "6. .\n",
    "7. ?\n",
    "8. ,\n",
    "9. \\<img title=\"smile\" alt=\"smile\" src=\"\\url.........}\" \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04814633",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501addc",
   "metadata": {},
   "source": [
    "Thanks, John, will, be, for, to, through, on, their, and, the ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8961570f",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fd826",
   "metadata": {},
   "source": [
    "Illustrations, demos, students, project, where, find"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a5a09",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff36db",
   "metadata": {},
   "source": [
    "html 문서의 태그로 사용되는 단어들이 용어로 인식되면 행렬이 비대해지고, 잡음이 생길 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8ccaa",
   "metadata": {},
   "source": [
    "# 20.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5edfe",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0809992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: import and label records\n",
    "corpus = []\n",
    "label = []\n",
    "with ZipFile('AutoAndElectronics.zip') as rawData:\n",
    "    for info in rawData.infolist():\n",
    "        if info.is_dir(): \n",
    "            continue\n",
    "        label.append(1 if 'rec.autos' in info.filename else 0)\n",
    "        corpus.append(rawData.read(info))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3abb5",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c2c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: preprocessing (tokenization, stemming, and stopwords)\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "preprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\n",
    "preprocessedText = preprocessor.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef04193",
   "metadata": {},
   "source": [
    "불필요하고 잡음이 많은 단어들이 다수 포함되면서 행렬이 비대해지고 예측성능이 떨어지고 모델형성에 시간이 많이 들 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b82760",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16992344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x13515 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 159748 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: TF-IDF and latent semantic analysis\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "309a462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract 20 concepts using LSA ()\n",
    "svd = TruncatedSVD(20)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cdce71",
   "metadata": {},
   "source": [
    "텍스트의 관리용이성을 크게 증진시키고 잡음을 줄여서 예측성능을 높일 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9d2e8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9575)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 389   8\n",
      "     1  26 377\n"
     ]
    }
   ],
   "source": [
    "# split dataset into 60% training and 40% test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=42)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracty\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdbea5",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f624091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9625)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 390   7\n",
      "     1  23 380\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# split dataset into 60% training and 40% test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=42)\n",
    "\n",
    "# run randomforest classifier model on training\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracy\n",
    "classificationSummary(ytest, forest.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349a472",
   "metadata": {},
   "source": [
    "accuracy가 0.9587이었던 rogistic 회귀보다 높은 성능을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58812257",
   "metadata": {},
   "source": [
    "# 20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6922c97",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c381fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 ad-jerry ad-bruckheimer ad-chase ad-premier ad-sept ad-th ad-clip ad-bruckheimer ad-chase page found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1 ad-rheumatoid ad-arthritis ad-expert ad-tip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1 ad-rheumatologist ad-anju ad-varghese ad-yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1 ad-siemen ad-water ad-remediation ad-water ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1 ad-symptom ad-muscle ad-weakness ad-genetic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 ad-animal ad-animal ad-wild ad-sa ad-officia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4137</th>\n",
       "      <td>-1 ad-affordable ad-ivf ad-cost ad-efficient a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>1 ad-mozypro ad-business ad-backup ad-affordab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139</th>\n",
       "      <td>1 ad-oster ad-line ad-clipper ad-oster ad-fact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4140</th>\n",
       "      <td>-1 ad-synrevoice ad-schoolconnect ad-trust ad-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>1 ad-vet ad-online ad-veterinarian ad-online a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4142 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1 ad-jerry ad-bruckheimer ad-chase ad-premier ad-sept ad-th ad-clip ad-bruckheimer ad-chase page found\n",
       "0     -1 ad-rheumatoid ad-arthritis ad-expert ad-tip...                                                    \n",
       "1     -1 ad-rheumatologist ad-anju ad-varghese ad-yo...                                                    \n",
       "2     -1 ad-siemen ad-water ad-remediation ad-water ...                                                    \n",
       "3     -1 ad-symptom ad-muscle ad-weakness ad-genetic...                                                    \n",
       "4     1 ad-animal ad-animal ad-wild ad-sa ad-officia...                                                    \n",
       "...                                                 ...                                                    \n",
       "4137  -1 ad-affordable ad-ivf ad-cost ad-efficient a...                                                    \n",
       "4138  1 ad-mozypro ad-business ad-backup ad-affordab...                                                    \n",
       "4139  1 ad-oster ad-line ad-clipper ad-oster ad-fact...                                                    \n",
       "4140  -1 ad-synrevoice ad-schoolconnect ad-trust ad-...                                                    \n",
       "4141  1 ad-vet ad-online ad-veterinarian ad-online a...                                                    \n",
       "\n",
       "[4142 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_csv(\"./farm-ads.csv\")\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2a37c",
   "metadata": {},
   "source": [
    "1 ad-animal ad-animal ad-wild ad-sa ad-officia...  \n",
    "1 ad-vet ad-online ad-veterinarian ad-online a...  \n",
    "-1 ad-rheumatoid ad-arthritis ad-expert ad-tip...  \n",
    "-1 ad-rheumatologist ad-anju ad-varghese ad-yo...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e51ca7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "preprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\n",
    "preprocessedText = preprocessor.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "392dd2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54ec857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(20)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e144f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          S1   S2   S3   S4   S5   S6   S7   S8   S9  S10  ...  S1991  S1992  \\\n",
      "aa       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
      "aaa      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
      "aaaaa    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
      "aaahh    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
      "aaltern  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
      "zurich   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
      "zwar     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
      "zx       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
      "zyxel    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
      "þ        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
      "\n",
      "         S1993  S1994  S1995  S1996  S1997  S1998  S1999  S2000  \n",
      "aa         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "aaa        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "aaaaa      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "aaahh      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "aaltern    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "...        ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "zurich     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "zwar       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "zx         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "zyxel      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "þ          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "\n",
      "[13515 rows x 2000 columns]\n"
     ]
    }
   ],
   "source": [
    "## tfidf\n",
    "\n",
    "printTermDocumentMatrix(preprocessor, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc7a8c8",
   "metadata": {},
   "source": [
    "### i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28df293c",
   "metadata": {},
   "source": [
    "희소행렬이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540398b",
   "metadata": {},
   "source": [
    "### ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17ad74e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               S1\n",
      "aa       0.000000\n",
      "aaa      0.000000\n",
      "aaaaa    0.000000\n",
      "aaahh    0.000000\n",
      "aaltern  0.000000\n",
      "...           ...\n",
      "zurich   0.000000\n",
      "zwar     0.000000\n",
      "zx       0.000000\n",
      "zyxel    0.000000\n",
      "þ        0.157242\n",
      "\n",
      "[13515 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "printTermDocumentMatrix(preprocessor, tfidf[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591094e3",
   "metadata": {},
   "source": [
    "p 문자가 S1문장에서 나온 빈도수와 모든 문장들에서 p가 들어간 문장의 비율의 곱이 0.157...이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38247af6",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d668ee",
   "metadata": {},
   "source": [
    "주성분분석에서 여러 변수들에서 최대한 정보 손실이 안 되도록 차원을 축소한 것 처럼, 여러 용어들을 최대한 그 용어들의 정보를 다 포함한 상태로 더 적은 개념들로 함축시킬 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6a62c",
   "metadata": {},
   "source": [
    "## c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c31ee20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9637)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 390   7\n",
      "     1  22 381\n"
     ]
    }
   ],
   "source": [
    "# split dataset into 60% training and 40% test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=42)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracty\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6279b2",
   "metadata": {},
   "source": [
    "유효성??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b686cc5",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8741beb8",
   "metadata": {},
   "source": [
    "텍스트의 관리용이성을 크게 증진시키고 잡음을 줄여서 예측성능을 높이기 위해서"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
