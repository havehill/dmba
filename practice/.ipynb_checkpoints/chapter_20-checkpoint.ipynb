{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e15590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/JAE111/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.snowball import EnglishStemmer \n",
    "import matplotlib.pylab as plt\n",
    "from dmba import printTermDocumentMatrix, classificationSummary, liftChart\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1793f",
   "metadata": {},
   "source": [
    "# Table 20.1 문장 S1~S3 내 단어들의 용어-문서 행렬 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071a70a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          S1  S2  S3\n",
      "first      1   0   0\n",
      "here       0   0   1\n",
      "is         1   1   1\n",
      "second     0   1   0\n",
      "sentence   1   1   1\n",
      "the        1   0   1\n",
      "third      0   0   1\n",
      "this       1   1   0\n"
     ]
    }
   ],
   "source": [
    "text = ['this is the first sentence.',\n",
    "       'this is a second sentence',\n",
    "       'the third sentence is here.']\n",
    "\n",
    "# Learn features based on text\n",
    "count_vect = CountVectorizer() # 각 term이 각 document에서 몇번 나오는지(빈도)를 추출하는 모델\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts) # term-document matrix를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97c211",
   "metadata": {},
   "source": [
    "# Table 20.2 Term-document representation of words in sentences S1-S4 (Example 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c552286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           S1  S2  S3  S4\n",
      "all         0   0   0   1\n",
      "first       1   0   0   0\n",
      "forth       0   0   0   1\n",
      "here        0   0   1   0\n",
      "is          1   1   1   0\n",
      "of          0   0   0   1\n",
      "second      0   1   0   0\n",
      "sentence    1   1   1   0\n",
      "sentences   0   0   0   1\n",
      "the         1   0   1   0\n",
      "third       0   0   1   0\n",
      "this        1   1   0   0\n"
     ]
    }
   ],
   "source": [
    "text = ['this is the first    sentence!!',\n",
    "       'this is a second Sentence :)',\n",
    "       'the third sentence, is here ',\n",
    "       'forth of all sentences']\n",
    "\n",
    "# Learn features based on text. Include special characters that are part of a word in the analysis.\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5823214d",
   "metadata": {},
   "source": [
    "# Table 20.3 Tokenization of S1-S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd597e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            S1  S2  S3  S4\n",
      ":)           0   1   0   0\n",
      "a            0   1   0   0\n",
      "all          0   0   0   1\n",
      "first        1   0   0   0\n",
      "forth        0   0   0   1\n",
      "here         0   0   1   0\n",
      "is           1   1   1   0\n",
      "of           0   0   0   1\n",
      "second       0   1   0   0\n",
      "sentence     0   1   1   0\n",
      "sentence!!   1   0   0   0\n",
      "sentences    0   0   0   1\n",
      "the          1   0   1   0\n",
      "third        0   0   1   0\n",
      "this         1   1   0   0\n"
     ]
    }
   ],
   "source": [
    "text = ['this is the first     sentence!!',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "\n",
    "count_vect = CountVectorizer(token_pattern = '[a-zA-Z!:)]+')\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3565c2",
   "metadata": {},
   "source": [
    "# Table 20.4 Stopwords in scitkit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e61c97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 180 of 318 stopwords\n",
      "a            about        above        across       after        afterwards   \n",
      "again        against      all          almost       alone        along        \n",
      "already      also         although     always       am           among        \n",
      "amongst      amoungst     amount       an           and          another      \n",
      "any          anyhow       anyone       anything     anyway       anywhere     \n",
      "are          around       as           at           back         be           \n",
      "became       because      become       becomes      becoming     been         \n",
      "before       beforehand   behind       being        below        beside       \n",
      "besides      between      beyond       bill         both         bottom       \n",
      "but          by           call         can          cannot       cant         \n",
      "co           con          could        couldnt      cry          de           \n",
      "describe     detail       do           done         down         due          \n",
      "during       each         eg           eight        either       eleven       \n",
      "else         elsewhere    empty        enough       etc          even         \n",
      "ever         every        everyone     everything   everywhere   except       \n",
      "few          fifteen      fifty        fill         find         fire         \n",
      "first        five         for          former       formerly     forty        \n",
      "found        four         from         front        full         further      \n",
      "get          give         go           had          has          hasnt        \n",
      "have         he           hence        her          here         hereafter    \n",
      "hereby       herein       hereupon     hers         herself      him          \n",
      "himself      his          how          however      hundred      i            \n",
      "ie           if           in           inc          indeed       interest     \n",
      "into         is           it           its          itself       keep         \n",
      "last         latter       latterly     least        less         ltd          \n",
      "made         many         may          me           meanwhile    might        \n",
      "mill         mine         more         moreover     most         mostly       \n",
      "move         much         must         my           myself       name         \n",
      "namely       neither      never        nevertheless next         nine         \n",
      "no           nobody       none         noone        nor          not          \n"
     ]
    }
   ],
   "source": [
    "stopWords = list(sorted(ENGLISH_STOP_WORDS)) \n",
    "# ENGLISH_STOP_WORDS ; scikit-learn 모듈 내에 저장되어 있는 stop-words(전처리 단계에서 제거되어야 하는 용어들 - 단순히 양과 잡음을 늘리는 용어들)\n",
    "ncolumns = 6; nrows = 30\n",
    "\n",
    "print('First {} of {} stopwords'.format(ncolumns * nrows, len(stopWords)))\n",
    "for i in range(0, len(stopWords[:(ncolumns * nrows)]), ncolumns):\n",
    "    print(''.join(word.ljust(13) for word in stopWords[i:(i+ncolumns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d9cf0f",
   "metadata": {},
   "source": [
    "# Table 20.5 Text reduction of S1-S4 using stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5992837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         S1  S2  S3  S4\n",
      "forth     0   0   0   1\n",
      "second    0   1   0   0\n",
      "sentenc   1   1   1   1\n"
     ]
    }
   ],
   "source": [
    "text = ['this is the first     sentence!! ',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "\n",
    "# Create a custom tokenizer that will use NLTK for tokenizing and lemmatizing\n",
    "# (removes interpunctuation and stop words)\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "# ⭐️⭐️⭐️이 코드 전체가 이해가 안 되는데요....??? 갑자기 클래스 정의?????? __init__???? __call__????⭐️⭐️⭐️\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=LemmaTokenizer())# 아까 정의한 토크나이저를 사용하여 토크나이징\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17085d51",
   "metadata": {},
   "source": [
    "# Table 20.6 tf-idf matrix for S1-S4 example (after tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59457c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 S1        S2        S3        S4\n",
      "all        0.000000  0.000000  0.000000  2.386294\n",
      "first      2.386294  0.000000  0.000000  0.000000\n",
      "forth      0.000000  0.000000  0.000000  2.386294\n",
      "here       0.000000  0.000000  2.386294  0.000000\n",
      "is         1.287682  1.287682  1.287682  0.000000\n",
      "of         0.000000  0.000000  0.000000  2.386294\n",
      "second     0.000000  2.386294  0.000000  0.000000\n",
      "sentence   1.287682  1.287682  1.287682  0.000000\n",
      "sentences  0.000000  0.000000  0.000000  2.386294\n",
      "the        1.693147  0.000000  1.693147  0.000000\n",
      "third      0.000000  0.000000  2.386294  0.000000\n",
      "this       1.693147  1.693147  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "text = ['this is the first     sentence!!',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "\n",
    "# Apply CountVectorizer and TfidfTransformer sequentially\n",
    "count_vect = CountVectorizer()\n",
    "tfidfTransformer = TfidfTransformer(smooth_idf = False, norm=None)\n",
    "# TfidfTransform : 셀의 값을 TermFrequency-InverseDocumentFrequency 값으로 바꿔주는 모델\n",
    "counts = count_vect.fit_transform(text)\n",
    "tfidf = tfidfTransformer.fit_transform(counts)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Table 20.6 tf-idf matrix for S1-S4 example (after tokenization)\n",
    "text = ['this is the first     sentence!!',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "​\n",
    "# Apply CountVectorizer and TfidfTransformer sequentially\n",
    "count_vect = CountVectorizer()\n",
    "tfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "counts = count_vect.fit_transform(text)\n",
    "tfidf = tfidfTransformer.fit_transform(counts)\n",
    "​\n",
    "printTermDocumentMatrix(count_vect, tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
